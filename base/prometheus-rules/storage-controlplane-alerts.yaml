# =============================================================================
# Storage Alert Rules
# =============================================================================
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: storage-alerts
  labels:
    app: kube-prometheus-stack
    release: robusta
spec:
  groups:
    - name: storage-health
      interval: 60s
      rules:
        - alert: PVCAlmostFull
          expr: |
            kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes > 0.85
          for: 5m
          labels:
            severity: warning
            category: storage
          annotations:
            summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} almost full"
            description: "PVC is {{ $value | humanizePercentage }} full"

        - alert: PVCCriticallyFull
          expr: |
            kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes > 0.95
          for: 2m
          labels:
            severity: critical
            category: storage
          annotations:
            summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} critically full"
            description: "PVC is {{ $value | humanizePercentage }} full - immediate action required"

        - alert: PVCPending
          expr: |
            kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
          for: 5m
          labels:
            severity: warning
            category: storage
          annotations:
            summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending"

        - alert: PVCLost
          expr: |
            kube_persistentvolumeclaim_status_phase{phase="Lost"} == 1
          for: 1m
          labels:
            severity: critical
            category: storage
          annotations:
            summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is lost"
            description: "Bound PV has been deleted - data may be lost"

        - alert: PVInodesFull
          expr: |
            kubelet_volume_stats_inodes_free / kubelet_volume_stats_inodes * 100 < 10
          for: 5m
          labels:
            severity: warning
            category: storage
          annotations:
            summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} running out of inodes"

---
# =============================================================================
# k3s Control Plane Alert Rules
# =============================================================================
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: control-plane-alerts
  labels:
    app: kube-prometheus-stack
    release: robusta
spec:
  groups:
    - name: k3s-control-plane
      interval: 30s
      rules:
        - alert: K3sAPIServerDown
          expr: |
            up{job="apiserver"} == 0
          for: 2m
          labels:
            severity: critical
            category: control-plane
          annotations:
            summary: "k3s API server is down"
            description: "The Kubernetes API server has been unreachable for more than 2 minutes"

        - alert: K3sAPIServerHighLatency
          expr: |
            histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{verb!~"WATCH|CONNECT"}[5m])) by (le, verb)) > 1
          for: 5m
          labels:
            severity: warning
            category: control-plane
          annotations:
            summary: "API server high latency"
            description: "99th percentile latency for {{ $labels.verb }} requests is {{ $value | printf \"%.2f\" }}s"

        - alert: K3sAPIServerHighErrorRate
          expr: |
            sum(rate(apiserver_request_total{code=~"5.."}[5m])) /
            sum(rate(apiserver_request_total[5m])) > 0.01
          for: 5m
          labels:
            severity: warning
            category: control-plane
          annotations:
            summary: "API server high error rate"
            description: "{{ $value | printf \"%.2f\" }}% of API requests are failing"

        - alert: K3sAPIServerClientErrors
          expr: |
            sum(rate(apiserver_request_total{code=~"4.."}[5m])) > 100
          for: 10m
          labels:
            severity: warning
            category: control-plane
          annotations:
            summary: "High rate of client errors to API server"
            description: "{{ $value | printf \"%.0f\" }} client errors per second"

        - alert: EtcdHighLatency
          expr: |
            histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) > 0.1
          for: 5m
          labels:
            severity: warning
            category: control-plane
          annotations:
            summary: "etcd high disk latency"
            description: "99th percentile WAL fsync latency is {{ $value | printf \"%.3f\" }}s"

        - alert: EtcdHighCommitLatency
          expr: |
            histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) > 0.25
          for: 5m
          labels:
            severity: warning
            category: control-plane
          annotations:
            summary: "etcd high commit latency"
            description: "99th percentile backend commit latency is {{ $value | printf \"%.3f\" }}s"

        - alert: EtcdDatabaseSizeGrowing
          expr: |
            predict_linear(etcd_mvcc_db_total_size_in_bytes[4h], 24*3600) > 8*1024*1024*1024
          for: 1h
          labels:
            severity: warning
            category: control-plane
          annotations:
            summary: "etcd database size growing"
            description: "etcd database will exceed 8GB within 24 hours at current growth rate"

        - alert: TooManyPods
          expr: |
            sum by(node) (kubelet_running_pods) / sum by(node) (kube_node_status_allocatable{resource="pods"}) > 0.9
          for: 10m
          labels:
            severity: warning
            category: resources
          annotations:
            summary: "Node {{ $labels.node }} has too many pods"
            description: "Node is running {{ $value | printf \"%.0f\" }}% of allocatable pods"

    - name: network-health
      interval: 30s
      rules:
        - alert: ServiceEndpointsDown
          expr: |
            kube_endpoint_address_available == 0
          for: 5m
          labels:
            severity: warning
            category: network
          annotations:
            summary: "Service {{ $labels.namespace }}/{{ $labels.endpoint }} has no endpoints"

        - alert: CoreDNSDown
          expr: |
            up{job="coredns"} == 0
          for: 2m
          labels:
            severity: critical
            category: network
          annotations:
            summary: "CoreDNS is down"

        - alert: CoreDNSHighLatency
          expr: |
            histogram_quantile(0.99, sum(rate(coredns_dns_request_duration_seconds_bucket[5m])) by (le)) > 0.5
          for: 5m
          labels:
            severity: warning
            category: network
          annotations:
            summary: "CoreDNS high latency"
            description: "99th percentile DNS latency is {{ $value | printf \"%.3f\" }}s"

    - name: certificates
      interval: 6h
      rules:
        - alert: CertificateExpiringSoon
          expr: |
            (certmanager_certificate_expiration_timestamp_seconds - time()) / 86400 < 30
          for: 1h
          labels:
            severity: warning
            category: security
          annotations:
            summary: "Certificate {{ $labels.namespace }}/{{ $labels.name }} expiring soon"
            description: "Certificate expires in {{ $value | printf \"%.0f\" }} days"

        - alert: CertificateExpiryCritical
          expr: |
            (certmanager_certificate_expiration_timestamp_seconds - time()) / 86400 < 7
          for: 1h
          labels:
            severity: critical
            category: security
          annotations:
            summary: "Certificate {{ $labels.namespace }}/{{ $labels.name }} expires in {{ $value | printf \"%.0f\" }} days"
