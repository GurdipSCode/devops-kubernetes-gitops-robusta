# Advanced Alert Rules for k3s
# Includes: SLOs, Security, Networking, Cost, k3s-specific alerts

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: k3s-advanced-alerts
  namespace: robusta
  labels:
    release: robusta
spec:
  groups:
    # ==========================================
    # SLO / SLI ALERTS
    # ==========================================
    - name: slo-alerts
      rules:
        # Error budget burn rate (Multi-window, Multi-burn-rate)
        # Fast burn - consuming budget quickly (page immediately)
        - alert: SLOErrorBudgetBurnFast
          expr: |
            (
              sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m])) > (14.4 * 0.001)
              and
              sum(rate(http_requests_total{status=~"5.."}[1h])) / sum(rate(http_requests_total[1h])) > (14.4 * 0.001)
            )
          for: 2m
          labels:
            severity: critical
            oncall: "true"
            slo: "true"
          annotations:
            summary: "SLO Error Budget burning fast - 2% budget consumed in 1 hour"
            description: "High error rate detected. At current rate, error budget will be exhausted."
            runbook_url: "https://wiki.internal/runbooks/slo-error-budget"

        # Slow burn - consuming budget gradually (ticket)
        - alert: SLOErrorBudgetBurnSlow
          expr: |
            (
              sum(rate(http_requests_total{status=~"5.."}[30m])) / sum(rate(http_requests_total[30m])) > (3 * 0.001)
              and
              sum(rate(http_requests_total{status=~"5.."}[6h])) / sum(rate(http_requests_total[6h])) > (3 * 0.001)
            )
          for: 15m
          labels:
            severity: warning
            create_ticket: "true"
            slo: "true"
          annotations:
            summary: "SLO Error Budget burning slowly - investigate during business hours"

        # Latency SLO (p99 < 500ms for 99.9% of requests)
        - alert: SLOLatencyBudgetBurn
          expr: |
            (
              histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 0.5
              and
              histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[1h])) by (le)) > 0.5
            )
          for: 5m
          labels:
            severity: warning
            slo: "true"
          annotations:
            summary: "Latency SLO at risk - p99 latency exceeds 500ms"

        # Availability SLO
        - alert: SLOAvailabilityBreach
          expr: |
            (1 - (sum(rate(http_requests_total{status=~"5.."}[1d])) / sum(rate(http_requests_total[1d])))) < 0.999
          for: 5m
          labels:
            severity: critical
            slo: "true"
          annotations:
            summary: "Daily availability below 99.9% SLO"

    # ==========================================
    # K3S CONTROL PLANE
    # ==========================================
    - name: k3s-control-plane
      rules:
        - alert: K3sAPIServerDown
          expr: up{job="apiserver"} == 0
          for: 1m
          labels:
            severity: critical
            oncall: "true"
          annotations:
            summary: "k3s API server is unreachable"

        - alert: K3sAPIServerHighErrorRate
          expr: |
            sum(rate(apiserver_request_total{code=~"5.."}[5m])) / sum(rate(apiserver_request_total[5m])) > 0.05
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "API server error rate above 5%"
            description: "Current error rate: {{ $value | humanizePercentage }}"

        - alert: K3sAPIServerLatencyHigh
          expr: |
            histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{verb!~"WATCH|CONNECT"}[5m])) by (le, verb, resource)) > 1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "API server p99 latency high for {{ $labels.verb }} {{ $labels.resource }}"
            description: "Latency: {{ $value | humanizeDuration }}"

        - alert: K3sAPIServerRequestsThrottled
          expr: rate(apiserver_dropped_requests_total[5m]) > 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "API server is throttling requests"

        - alert: K3sEtcdHighLatency
          expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) > 0.5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "etcd disk fsync latency is high"
            description: "p99 latency: {{ $value | humanizeDuration }}"

        - alert: K3sEtcdHighCommitLatency
          expr: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) > 0.25
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "etcd commit latency is high"

        - alert: K3sEtcdDatabaseSizeLarge
          expr: etcd_server_quota_backend_bytes - etcd_mvcc_db_total_size_in_bytes < 100*1024*1024
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "etcd database approaching quota limit"
            description: "Less than 100MB remaining before quota"

        - alert: K3sControllerManagerDown
          expr: up{job="kube-controller-manager"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Controller manager is down"

        - alert: K3sSchedulerDown
          expr: up{job="kube-scheduler"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Scheduler is down"

        # k3s specific - embedded components
        - alert: K3sAgentNotReady
          expr: k3s_agent_ready == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "k3s agent not ready on {{ $labels.node }}"

    # ==========================================
    # NODE ALERTS - ADVANCED
    # ==========================================
    - name: node-alerts-advanced
      rules:
        - alert: NodeHighCPU
          expr: 100 - (avg by(node, instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High CPU usage on {{ $labels.node }}"
            description: "CPU: {{ $value | printf \"%.1f\" }}%"

        - alert: NodeHighCPUCritical
          expr: 100 - (avg by(node, instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
          for: 2m
          labels:
            severity: critical
            oncall: "true"
          annotations:
            summary: "Critical CPU usage on {{ $labels.node }}"

        - alert: NodeHighMemory
          expr: (1 - node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) * 100 > 85
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High memory on {{ $labels.node }}"
            description: "Memory: {{ $value | printf \"%.1f\" }}%"

        - alert: NodeHighMemoryCritical
          expr: (1 - node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) * 100 > 95
          for: 2m
          labels:
            severity: critical
            oncall: "true"
          annotations:
            summary: "Critical memory on {{ $labels.node }}"

        - alert: NodeDiskPressure
          expr: (1 - node_filesystem_avail_bytes{fstype!~"tmpfs|overlay",mountpoint="/"}/node_filesystem_size_bytes) * 100 > 80
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Disk pressure on {{ $labels.node }}"
            description: "Root filesystem: {{ $value | printf \"%.1f\" }}% used"

        - alert: NodeDiskPressureCritical
          expr: (1 - node_filesystem_avail_bytes{fstype!~"tmpfs|overlay",mountpoint="/"}/node_filesystem_size_bytes) * 100 > 90
          for: 2m
          labels:
            severity: critical
            oncall: "true"
          annotations:
            summary: "Critical disk pressure on {{ $labels.node }}"

        - alert: NodeDiskWillFillIn24Hours
          expr: |
            predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"}[6h], 24*60*60) < 0
          for: 30m
          labels:
            severity: warning
          annotations:
            summary: "Disk on {{ $labels.node }} will fill within 24 hours"

        - alert: NodeHighLoadAverage
          expr: node_load15 / count without(cpu, mode) (node_cpu_seconds_total{mode="idle"}) > 2
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "High load average on {{ $labels.node }}"
            description: "15m load average per CPU: {{ $value | printf \"%.2f\" }}"

        - alert: NodeNetworkReceiveErrors
          expr: rate(node_network_receive_errs_total{device!~"veth.*|cali.*|flannel.*"}[5m]) > 10
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Network receive errors on {{ $labels.node }} ({{ $labels.device }})"

        - alert: NodeNetworkTransmitErrors
          expr: rate(node_network_transmit_errs_total{device!~"veth.*|cali.*|flannel.*"}[5m]) > 10
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Network transmit errors on {{ $labels.node }} ({{ $labels.device }})"

        - alert: NodeClockSkew
          expr: abs(node_timex_offset_seconds) > 0.05
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Clock skew detected on {{ $labels.node }}"
            description: "Offset: {{ $value | humanizeDuration }}"

        - alert: NodeNotReady
          expr: kube_node_status_condition{condition="Ready",status="true"} == 0
          for: 2m
          labels:
            severity: critical
            oncall: "true"
          annotations:
            summary: "Node {{ $labels.node }} is not ready"

        - alert: NodeUnschedulable
          expr: kube_node_spec_unschedulable == 1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Node {{ $labels.node }} is cordoned"

        - alert: NodePIDPressure
          expr: kube_node_status_condition{condition="PIDPressure",status="true"} == 1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "PID pressure on node {{ $labels.node }}"

        # k3s containerd specific
        - alert: NodeContainerdDown
          expr: up{job="containerd"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "containerd is down on {{ $labels.node }}"

    # ==========================================
    # POD / WORKLOAD ALERTS - ADVANCED
    # ==========================================
    - name: workload-alerts-advanced
      rules:
        - alert: PodCrashLooping
          expr: increase(kube_pod_container_status_restarts_total[1h]) > 3
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} crash looping"
            description: "{{ $value | printf \"%.0f\" }} restarts in last hour"

        - alert: PodNotReady
          expr: |
            sum by(namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown"}) > 0
            unless on(namespace, pod) kube_pod_owner{owner_kind="Job"}
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} not ready"

        - alert: PodOOMKilled
          expr: kube_pod_container_status_last_terminated_reason{reason="OOMKilled"} == 1
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: "Container {{ $labels.container }} OOMKilled in {{ $labels.namespace }}/{{ $labels.pod }}"

        - alert: ContainerHighCPUThrottling
          expr: |
            sum by(namespace, pod, container) (
              rate(container_cpu_cfs_throttled_periods_total[5m])
            ) / sum by(namespace, pod, container) (
              rate(container_cpu_cfs_periods_total[5m])
            ) > 0.5
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Container {{ $labels.container }} in {{ $labels.namespace }}/{{ $labels.pod }} CPU throttled"
            description: "Throttled {{ $value | humanizePercentage }} of periods"

        - alert: ContainerMemoryNearLimit
          expr: |
            container_memory_working_set_bytes{container!="",container!="POD"}
            / on(namespace, pod, container) kube_pod_container_resource_limits{resource="memory"}
            > 0.9
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Container {{ $labels.container }} memory near limit"
            description: "Using {{ $value | humanizePercentage }} of memory limit"

        - alert: DeploymentReplicasMismatch
          expr: |
            kube_deployment_spec_replicas != kube_deployment_status_replicas_available
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replica mismatch"
            description: "Expected {{ $labels.spec_replicas }}, available {{ $labels.status_replicas }}"

        - alert: DeploymentHighCPU
          expr: |
            sum by(namespace, deployment) (
              rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[5m])
              * on(namespace, pod) group_left(deployment)
              label_replace(kube_pod_owner{owner_kind="ReplicaSet"}, "replicaset", "$1", "owner_name", "(.*)")
              * on(namespace, replicaset) group_left(deployment)
              label_replace(kube_replicaset_owner{owner_kind="Deployment"}, "deployment", "$1", "owner_name", "(.*)")
            ) / sum by(namespace, deployment) (
              kube_deployment_spec_replicas
              * on(namespace, deployment) group_left()
              avg by(namespace, deployment) (
                kube_pod_container_resource_requests{resource="cpu"}
                * on(namespace, pod) group_left(deployment)
                label_replace(kube_pod_owner{owner_kind="ReplicaSet"}, "replicaset", "$1", "owner_name", "(.*)")
                * on(namespace, replicaset) group_left(deployment)
                label_replace(kube_replicaset_owner{owner_kind="Deployment"}, "deployment", "$1", "owner_name", "(.*)")
              )
            ) > 0.85
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} high CPU"

        - alert: DeploymentLowUtilization
          expr: |
            sum by(namespace, deployment) (
              rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[1h])
            ) / sum by(namespace, deployment) (
              kube_pod_container_resource_requests{resource="cpu"}
            ) < 0.2
          for: 1h
          labels:
            severity: info
          annotations:
            summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} underutilized"
            description: "CPU utilization below 20% for 1 hour"

        - alert: StatefulSetReplicasMismatch
          expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} replica mismatch"

        - alert: DaemonSetNotScheduled
          expr: |
            kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_number_ready > 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has unready pods"

        - alert: JobFailed
          expr: kube_job_status_failed > 0
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: "Job {{ $labels.namespace }}/{{ $labels.job_name }} failed"

        - alert: CronJobSuspended
          expr: kube_cronjob_spec_suspend == 1
          for: 5m
          labels:
            severity: info
          annotations:
            summary: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is suspended"

        - alert: CronJobNotScheduled
          expr: |
            time() - kube_cronjob_status_last_schedule_time > kube_cronjob_spec_starting_deadline_seconds
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} missed schedule"

    # ==========================================
    # STORAGE ALERTS
    # ==========================================
    - name: storage-alerts
      rules:
        - alert: PVCAlmostFull
          expr: |
            kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes > 0.85
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} almost full"
            description: "{{ $value | humanizePercentage }} used"

        - alert: PVCCriticallyFull
          expr: |
            kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes > 0.95
          for: 2m
          labels:
            severity: critical
            oncall: "true"
          annotations:
            summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} critically full"

        - alert: PVCInodesFull
          expr: |
            kubelet_volume_stats_inodes_used / kubelet_volume_stats_inodes > 0.9
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} running out of inodes"

        - alert: PVCPending
          expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} stuck pending"

        - alert: PVLost
          expr: kube_persistentvolume_status_phase{phase="Failed"} == 1
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "PV {{ $labels.persistentvolume }} in failed state"

        - alert: UnusedPVC
          expr: |
            kube_persistentvolumeclaim_status_phase{phase="Bound"} == 1
            unless on(namespace, persistentvolumeclaim) (
              kube_pod_spec_volumes_persistentvolumeclaims_info
            )
          for: 24h
          labels:
            severity: info
            create_ticket: "true"
          annotations:
            summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} unused for 24h"

        # k3s local-path-provisioner specific
        - alert: LocalPathProvisionerDown
          expr: up{job=~".*local-path.*"} == 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "local-path-provisioner is down"

    # ==========================================
    # NETWORKING ALERTS
    # ==========================================
    - name: networking-alerts
      rules:
        - alert: ServiceEndpointsDown
          expr: |
            kube_endpoint_address_available == 0
            unless on(namespace, endpoint) kube_endpoint_address_not_ready > 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Service {{ $labels.namespace }}/{{ $labels.endpoint }} has no endpoints"

        - alert: IngressHighErrorRate
          expr: |
            sum by(ingress, namespace) (
              rate(nginx_ingress_controller_requests{status=~"5.."}[5m])
            ) / sum by(ingress, namespace) (
              rate(nginx_ingress_controller_requests[5m])
            ) > 0.05
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Ingress {{ $labels.namespace }}/{{ $labels.ingress }} high error rate"
            description: "{{ $value | humanizePercentage }} errors"

        # Traefik (k3s default) alerts
        - alert: TraefikHighErrorRate
          expr: |
            sum(rate(traefik_entrypoint_requests_total{code=~"5.."}[5m]))
            / sum(rate(traefik_entrypoint_requests_total[5m])) > 0.05
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Traefik high error rate: {{ $value | humanizePercentage }}"

        - alert: TraefikHighLatency
          expr: |
            histogram_quantile(0.99, sum(rate(traefik_entrypoint_request_duration_seconds_bucket[5m])) by (le, entrypoint)) > 1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Traefik p99 latency high on {{ $labels.entrypoint }}"

        - alert: TraefikBackendDown
          expr: traefik_backend_server_up == 0
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "Traefik backend {{ $labels.backend }} is down"

        - alert: CoreDNSDown
          expr: up{job="coredns"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "CoreDNS is down"

        - alert: CoreDNSHighLatency
          expr: |
            histogram_quantile(0.99, sum(rate(coredns_dns_request_duration_seconds_bucket[5m])) by (le)) > 0.1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "CoreDNS p99 latency above 100ms"

        - alert: CoreDNSHighErrorRate
          expr: |
            sum(rate(coredns_dns_responses_total{rcode=~"SERVFAIL|REFUSED"}[5m]))
            / sum(rate(coredns_dns_responses_total[5m])) > 0.05
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "CoreDNS high error rate"

        - alert: NetworkPolicyDenyingTraffic
          expr: |
            sum by(namespace) (
              increase(cilium_drop_count_total{reason="POLICY_DENIED"}[5m])
            ) > 100
          for: 5m
          labels:
            severity: info
          annotations:
            summary: "Network policy denying traffic in {{ $labels.namespace }}"

    # ==========================================
    # SECURITY ALERTS
    # ==========================================
    - name: security-alerts
      rules:
        - alert: PrivilegedContainerRunning
          expr: |
            sum by(namespace, pod, container) (
              kube_pod_container_info
              * on(namespace, pod) group_left()
              (kube_pod_spec_containers_security_context_privileged == 1)
            ) > 0
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: "Privileged container {{ $labels.container }} in {{ $labels.namespace }}/{{ $labels.pod }}"

        - alert: ContainerRunningAsRoot
          expr: |
            sum by(namespace, pod, container) (
              kube_pod_container_info
              * on(namespace, pod) group_left()
              (kube_pod_spec_containers_security_context_run_as_user == 0)
            ) > 0
          for: 5m
          labels:
            severity: info
          annotations:
            summary: "Container {{ $labels.container }} running as root in {{ $labels.namespace }}/{{ $labels.pod }}"

        - alert: PodWithHostNetwork
          expr: kube_pod_spec_host_network == 1
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} using host network"

        - alert: PodWithHostPID
          expr: kube_pod_spec_host_pid == 1
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} using host PID namespace"

        - alert: SecretAccessAnomaly
          expr: |
            sum by(user) (
              increase(apiserver_request_total{resource="secrets",verb=~"get|list"}[5m])
            ) > 100
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High secret access rate by {{ $labels.user }}"

        - alert: TooManyClusterAdmins
          expr: |
            count(
              kube_clusterrolebinding_info{clusterrolebinding=~".*admin.*"}
            ) > 5
          for: 1h
          labels:
            severity: info
          annotations:
            summary: "More than 5 cluster admin bindings detected"

        - alert: ServiceAccountTokenAutoMounted
          expr: |
            sum(kube_pod_spec_automount_service_account_token) > 0
          for: 5m
          labels:
            severity: info
          annotations:
            summary: "Service account tokens auto-mounted in some pods"

    # ==========================================
    # COST OPTIMIZATION ALERTS
    # ==========================================
    - name: cost-optimization
      rules:
        - alert: OverprovisionedDeployment
          expr: |
            (
              sum by(namespace, deployment) (
                avg_over_time(container_cpu_usage_seconds_total{container!="",container!="POD"}[24h])
              ) / sum by(namespace, deployment) (
                kube_pod_container_resource_requests{resource="cpu"}
              )
            ) < 0.3
          for: 24h
          labels:
            severity: info
            create_ticket: "true"
          annotations:
            summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} CPU over-provisioned"
            description: "Using only {{ $value | humanizePercentage }} of requested CPU"

        - alert: MemoryOverprovisioned
          expr: |
            (
              sum by(namespace, deployment) (
                avg_over_time(container_memory_working_set_bytes{container!="",container!="POD"}[24h])
              ) / sum by(namespace, deployment) (
                kube_pod_container_resource_requests{resource="memory"}
              )
            ) < 0.3
          for: 24h
          labels:
            severity: info
            create_ticket: "true"
          annotations:
            summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} memory over-provisioned"

        - alert: OrphanedReplicaSet
          expr: |
            kube_replicaset_spec_replicas == 0
            and on(namespace, replicaset) kube_replicaset_owner{owner_kind="Deployment"}
          for: 24h
          labels:
            severity: info
          annotations:
            summary: "Orphaned ReplicaSet {{ $labels.namespace }}/{{ $labels.replicaset }}"

        - alert: UnusedConfigMap
          expr: |
            kube_configmap_info
            unless on(namespace, configmap) (
              kube_pod_spec_volumes_configmap_info
            )
          for: 7d
          labels:
            severity: info
          annotations:
            summary: "ConfigMap {{ $labels.namespace }}/{{ $labels.configmap }} unused for 7 days"

        - alert: UnusedSecret
          expr: |
            kube_secret_info{type!="kubernetes.io/service-account-token"}
            unless on(namespace, secret) (
              kube_pod_spec_volumes_secret_info
            )
          for: 7d
          labels:
            severity: info
          annotations:
            summary: "Secret {{ $labels.namespace }}/{{ $labels.secret }} unused for 7 days"
